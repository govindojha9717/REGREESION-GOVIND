{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY4W8B8V3oEl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical Questions Regression\n",
        "Q1. What is Simple Linear Regression?\n",
        "\n",
        "---> Simple Linear Regression is a statistical method used to model the relationship between:\n",
        "\n",
        "One independent variable (X)\n",
        "\n",
        "One dependent variable (Y)\n",
        "\n",
        "It assumes a linear relationship: Y = mX + c\n",
        "\n",
        "Where:\n",
        "\n",
        "Y = Predicted value (target)\n",
        "\n",
        "X = Input feature\n",
        "\n",
        "m = Slope (how much Y changes with X)\n",
        "\n",
        "c = Intercept (value of Y when X = 0)\n",
        "\n",
        "Q2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "--->To ensure that Simple Linear Regression gives reliable and accurate results, a few assumptions must be met:\n",
        "\n",
        "Linearity The relationship between the independent variable (X) and the dependent variable (Y) is linear.\n",
        "\n",
        "Independence of Errors Residuals (errors) should be independent of each other. Important when data is time-based (use Durbin-Watson test if needed).\n",
        "\n",
        "Homoscedasticity (Constant Variance of Errors) The spread of residuals should be constant across all values of X. If residuals \"fan out\" or \"shrink\", the model may be biased.\n",
        "\n",
        "Normality of Errors The residuals should be normally distributed (especially for confidence intervals and hypothesis testing). Check with a histogram or Q-Q plot.\n",
        "\n",
        "No Multicollinearity (Only applies to multiple regression)\n",
        "\n",
        "In simple linear regression, there's only one independent variable, so this isn't a concern.\n",
        "\n",
        "Q3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "--->m represents the rate of change of Y with respect to X.\n",
        "\n",
        "In simple terms:\n",
        "\n",
        "If m = 2, then for every 1 unit increase in X, Y increases by 2 units.\n",
        "\n",
        "If m is negative, Y decreases as X increases.\n",
        "\n",
        "Q4. What does the intercept c represent in the equation Y=mX+c?\n",
        "\n",
        "--->Y = mX + c\n",
        "\n",
        "c is the value of Y when X = 0. It's where the line crosses the Y-axis.\n",
        "\n",
        "Q5. How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "---> m = Covariance(X, Y) Ã· Variance(X)"
      ],
      "metadata": {
        "id": "dV4gVWLy3rhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5])\n",
        "Y = np.array([2, 4, 5, 4, 5])\n",
        "\n",
        "n = len(X)\n",
        "m = (n * np.sum(X*Y) - np.sum(X)*np.sum(Y)) / (n*np.sum(X**2) - (np.sum(X))**2)\n",
        "print(\"Slope (m):\", m)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmoUD7cS3whC",
        "outputId": "a30edf09-3e42-422c-fd75-9ddcc9a88c34"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope (m): 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a89xcAJQ30wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "--->The least squares method finds the best-fitting line by minimizing the total squared error between the predicted values and actual data points.\n",
        "\n",
        "When we fit a regression line (Y = mX + c), it's rarely a perfect match to all data points.\n",
        "\n",
        "So, for each point, there's a residual (error):\n",
        "\n",
        "Residual=Actual Yâˆ’Predicted Y Least Squares finds the values of m (slope) and c (intercept) that minimize the sum of squares of these residuals.\n",
        "\n",
        "Q7. How is the coefficient of determination (RÂ²) interpreted in Simple Linear Regression?\n",
        "\n",
        "--->RÂ² is a statistical measure that tells you how well the regression line fits the data. It is also called the coefficient of determination.\n",
        "\n",
        "RÂ² explains how much of the variation in the dependent variable (Y) is explained by the independent variable (X) using the regression line.\n",
        "\n",
        "Q8. What is Multiple Linear Regression?\n",
        "\n",
        "--->Multiple Linear Regression is an extension of Simple Linear Regression where the model uses two or more independent variables (features) to predict a single dependent variable (target).\n",
        "\n",
        "It helps you predict an outcome (Y) based on multiple inputs (Xâ‚, Xâ‚‚, ..., Xn).\n",
        "\n",
        "Q9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "--->Simple: Predicting salary based on years of experience\n",
        "\n",
        "Multiple: Predicting salary based on experience, education, and city\n",
        "\n",
        "Type\tBest Used When...\n",
        "Simple Regression\tYou have one feature influencing the outcome\n",
        "Multiple Regression\tYou have many features impacting the outcome\n",
        "Q10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "--->To ensure that Multiple Linear Regression gives reliable and valid results, several key assumptions must be met:\n",
        "\n",
        "Linearity The relationship between the dependent variable and each independent variable is linear.\n",
        "Also assumes additive effects (i.e., the effect of each feature adds up).\n",
        "\n",
        "2.Independence of Errors (Residuals) The residuals (prediction errors) should be independent of each other.\n",
        "\n",
        "Particularly important in time series data.\n",
        "\n",
        "Homoscedasticity The residuals should have constant variance across all levels of predicted values.\n",
        "\n",
        "Normality of Residuals The residuals should be normally distributed for accurate confidence intervals and hypothesis testing.\n",
        "\n",
        "No Multicollinearity The independent variables should not be too highly correlated with each other.\n",
        "\n",
        "Multicollinearity makes it hard to determine the effect of each feature.\n",
        "\n",
        "No Influential Outliers No single data point should have a disproportionate impact on the model.\n",
        "Q11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "---> Heteroscedasticity refers to a situation in regression analysis where the variance of the residuals (errors) is not constant across all levels of the independent variables.\n",
        "\n",
        "The spread of errors increases or decreases as the predicted values change.\n",
        "\n",
        "This violates one of the key assumptions of Multiple Linear Regression, which expects homoscedasticity (constant variance of residuals).\n",
        "\n",
        "Q12.How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "--->Multicollinearity occurs when two or more independent variables are highly correlated with each other. This makes it hard for the model to accurately estimate the effect of each variable."
      ],
      "metadata": {
        "id": "FXXa_dSW30-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Assuming X is a DataFrame of features\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]"
      ],
      "metadata": {
        "id": "8CaJ1m7j5Aaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "--->Categorical variables (like Gender, City, Color) must be converted into numerical form before being used in regression models. Here are the most widely used techniques:\n",
        "\n",
        "One-Hot Encoding Converts each category into a separate binary (0/1) column.\n",
        "Best for nominal data (no inherent order).\n",
        "\n",
        "Label Encoding Assigns each category a unique number (0, 1, 2...).\n",
        "Suitable for ordinal data (where order matters)\n",
        "\n",
        "Ordinal Encoding Manually assigns ordered integers based on real-world order\n",
        "\n",
        "Target Encoding (Mean Encoding) Replace each category with the mean of the target variable for that category\n",
        "\n",
        "Binary Encoding / Hashing Useful when you have high-cardinality (many unique categories)\n",
        "\n",
        "Reduces dimensionality compared to One-Hot\n",
        "\n",
        "Q14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "--->Interaction terms are used in multiple linear regression when the effect of one independent variable on the dependent variable depends on the value of another independent variable.\n",
        "\n",
        "An interaction term captures the combined effect of two features that may not be apparent when each is considered alone.\n",
        "\n",
        "Q15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "--->The intercept is the predicted value of the dependent variable (Y) when all independent variables (Xâ€™s) are equal to zero.\n",
        "\n",
        "In Simple Linear Regression, the intercept often has a clear, practical meaning.\n",
        "\n",
        "In Multiple Linear Regression, it may be theoretically necessary but often not useful or interpretable on its own.\n",
        "\n",
        "Q16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "--->In regression analysis (Simple or Multiple), the slope (also called coefficient or weight) measures:\n",
        "\n",
        "âž¤ The change in the dependent variable (Y) for a one-unit increase in the independent variable (X), keeping other variables constant.\n",
        "\n",
        "When using regression:\n",
        "\n",
        "We test whether a slope is significantly different from 0\n",
        "\n",
        "A p-value < 0.05 usually indicates the slope is statistically significant â†’ i.e., that variable has a meaningful impact on the target\n",
        "\n",
        "Q17.How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "--->In a regression model (simple or multiple), the intercept is the value of the dependent variable (Y) when all independent variables (Xâ‚, Xâ‚‚, ..., Xn) are equal to 0. The intercept sets the baseline or starting point for predictions. It helps you understand:\n",
        "\n",
        "What the outcome would be if all features had zero influence.\n",
        "\n",
        "When It Might Not Be Useful: If no real-world scenario exists where all Xs are zero.\n",
        "\n",
        "Then, intercept becomes theoretical but still necessary for accurate prediction.\n",
        "\n",
        "The intercept provides the starting value of the prediction and puts the coefficients (slopes) into context. Itâ€™s most useful when X = 0 is meaningful, but itâ€™s always mathematically essential for accurate regression modeling.\n",
        "\n",
        "Q18. What are the limitations of using RÂ² as a sole measure of model performance?\n",
        "\n",
        "--->While RÂ² (coefficient of determination) is a popular metric in regression analysis, it has several important limitations when used alone:\n",
        "\n",
        "Does Not Indicate Model Accuracy A high RÂ² does not mean that your predictions are close to actual values.\n",
        "It only tells you how well the model explains the variance, not how well it predicts.\n",
        "\n",
        "Does Not Detect Overfitting RÂ² always increases as you add more features â€” even if those features are irrelevant.\n",
        "This can lead to overfitting, where the model performs well on training data but poorly on new data.\n",
        "\n",
        "Cannot Compare Different Types of Models RÂ² is specific to linear regression.\n",
        "You canâ€™t meaningfully compare RÂ² values from:\n",
        "\n",
        "Linear vs. non-linear models\n",
        "\n",
        "Different dependent variables\n",
        "\n",
        "Sensitive to Outliers A few extreme values can skew RÂ² dramatically, making it seem better or worse than it is.\n",
        "\n",
        "Doesnâ€™t Tell You Whether the Model Is Biased A model could have a decent RÂ² but still be biased (e.g., systematically underpredicting or overpredicting).\n",
        "\n",
        "RÂ² is useful, but using it alone is risky. Combine it with error metrics, plots, and validation to truly evaluate your model.\n",
        "\n",
        "Q19. How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "--->The standard error (SE) of a regression coefficient measures the variability or uncertainty in the estimate of that coefficient.\n",
        "\n",
        "A large SE means that the estimate of the coefficient is not precise â€” it could vary a lot if you repeated the model on different data samples.\n",
        "\n",
        "How to Interpret It (Practically): Suppose your model outputs:\n",
        "\n",
        "Coefficient for X=2.5,Standard Error=3.2 That means:\n",
        "\n",
        "You're not very confident that 2.5 is the true effect of X.\n",
        "\n",
        "In fact, the true effect might be zero or even negative.\n",
        "\n",
        "Q20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "--->Heteroscedasticity refers to the situation where the variance of residuals (errors) is not constant across all levels of the independent variable(s) or predicted values. This violates a key assumption of linear regression: constant variance (called homoscedasticity).\n",
        "\n",
        "Pattern\tWhat It Means\n",
        "ðŸ”º Funnel shape (spread increases)\tResiduals become more variable â†’ Heteroscedasticity\n",
        "ðŸ”» Inverted funnel (spread decreases)\tLess variation at higher predictions â†’ Also heteroscedastic\n",
        "ðŸŽ¯ Curved or structured pattern\tModel may be mis-specified (missing interaction or nonlinearity)\n",
        "Why It's Important to Address Heteroscedasticity:\n",
        "\n",
        "Problem\tWhy It Matters\n",
        "âŒ Biased standard errors\tConfidence intervals & p-values become unreliable\n",
        "âŒ Poor inference\tYou may draw wrong conclusions about feature importance\n",
        "âŒ Inefficient estimators\tModel may not be optimal or generalize well\n",
        "âŒ Affects trust in predictions\tEspecially if predictions are used for decisions\n",
        "Heteroscedasticity weakens the trustworthiness of your model's statistical outputs. Identifying it through residual plots and correcting it is crucial for building valid, robust regression models.\n",
        "\n",
        "Q21. What does it mean if a Multiple Linear Regression model has a high RÂ² but low adjusted RÂ²?\n",
        "\n",
        "--->| Term | Meaning | | ------------------ | -------------------------------------------------------- | | RÂ² (R-squared) | % of variance in the target (Y) explained by the model | | Adjusted RÂ² | RÂ² adjusted for the number of predictors used |\n",
        "\n",
        "A high RÂ² and low Adjusted RÂ² means the model appears to perform well, but the added variables may not actually improve it â€” they might just be noise.\n",
        "\n",
        "Q22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "--->Feature scaling is the process of standardizing or normalizing numerical variables so that they are on the same scale, typically:\n",
        "\n",
        "Standardization: mean = 0, std = 1\n",
        "\n",
        "Normalization: range = [0, 1]\n",
        "\n",
        "Interpretability of Coefficients Without scaling, large-scale variables (like salary in â‚¹ lakhs) can dominate smaller-scale ones (like age).\n",
        "Coefficients will reflect scale, not importance.\n",
        "\n",
        "Numerical Stability If features differ in magnitude significantly, matrix operations (like calculating inverse of ð‘‹ ð‘‡ ð‘‹ X T X) can be unstable.\n",
        "Leads to poor or incorrect model estimation.\n",
        "\n",
        "3.Regularization Requires Scaling Techniques like Ridge and Lasso Regression (which penalize large coefficients) are scale-sensitive.\n",
        "\n",
        "Without scaling, penalties unfairly affect large-valued features.\n",
        "\n",
        "Faster Convergence in Optimization Some solvers (like gradient descent) converge slowly if features vary wildly in scale.\n",
        "Scaling helps models train faster and more smoothly.\n",
        "\n",
        "When It May Not Be Necessary: Scaling is less critical for plain Ordinary Least Squares (OLS) regression when:\n",
        "\n",
        "You donâ€™t use regularization\n",
        "\n",
        "You don't care about comparing feature importance\n",
        "\n",
        "You know all features are on similar scales\n",
        "\n",
        "Q23. What is polynomial regression?\n",
        "\n",
        "--->Polynomial Regression is a form of regression analysis where the relationship between the independent variable(s) and the dependent variable is modeled as an nth-degree polynomial.\n",
        "\n",
        "It extends linear regression by adding non-linear terms (squared, cubic, etc.) of the predictors â€” but the model is still linear in the coefficients.\n",
        "\n",
        "Q24. How does polynomial regression differ from linear regression?\n",
        "\n",
        "--->\n",
        "\n",
        "Feature\tLinear Regression\tPolynomial Regression\n",
        "Model Equation\n",
        "Type of Relationship\tModels linear relationships\tModels non-linear relationships\n",
        "Curve Fit\tStraight line\tCurved line (U-shape, S-shape, etc.)\n",
        "Complexity\tSimple\tIncreases with polynomial degree\n",
        "Model Type\tLinear in both X and coefficients\tNon-linear in X, but linear in coefficients\n",
        "Risk\tMay underfit if data is non-linear\tMay overfit if degree is too high\n",
        "Feature Engineering\tUses raw features only\tIncludes powers of features (e.g.,\n",
        ",\n",
        ")\n",
        "Use Linear Regression for straight-line trends. Use Polynomial Regression when your data follows a curved pattern that can't be captured with a simple line.\n",
        "\n",
        "Q25. When is polynomial regression used?\n",
        "\n",
        "--->Polynomial Regression is used when the relationship between the independent variable(s) and the dependent variable is non-linear, but can still be modeled using a polynomial function.\n",
        "\n",
        "Use Polynomial Regression when your data shows non-linear patterns that can be captured using powers of the independent variable â€” but be careful to balance model flexibility with overfitting.\n",
        "\n",
        "Q26. What is the general equation for polynomial regression?\n",
        "\n",
        "--->Polynomial regression models the relationship between the dependent variable Y and one or more independent variables X as a polynomial function.\n",
        "\n",
        "Term\tMeaning\n",
        "Intercept\n",
        "Coefficients for polynomial terms\n",
        "Input variable raised to power\n",
        "Random error / noise\n",
        "Q27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "--->Yes â€” Polynomial Regression Can Be Applied to Multiple Variables\n",
        "\n",
        "This is called Multivariate Polynomial Regression. It allows you to model non-linear relationships between the dependent variable and two or more independent variables, including their interactions and higher-degree terms.\n",
        "\n",
        "Q28. What are the limitations of polynomial regression?\n",
        "\n",
        "--->Limitations of Polynomial Regression While Polynomial Regression is a powerful tool for modeling non-linear relationships, it comes with several important limitations you should understand before using it.\n",
        "\n",
        "Overfitting (Too Much Flexibility) As the degree increases, the model becomes more complex and starts fitting noise instead of the true pattern.\n",
        "This results in poor generalization to new data.\n",
        "\n",
        "Example: A degree-10 polynomial may fit training data perfectly, but perform terribly on test data.\n",
        "\n",
        "Computational Complexity Adding higher-degree terms and multiple variables creates a large number of features.\n",
        "This increases the training time and can cause numerical instability.\n",
        "\n",
        "Loss of Interpretability In higher-degree models, it's hard to understand or explain what each term is doing.\n",
        "Interpretability is important in many domains (e.g., healthcare, finance). 4. Extrapolation is Dangerous Predictions outside the range of training data can be wildly inaccurate.\n",
        "\n",
        "Polynomials tend to explode to extreme values beyond the known data range.\n",
        "\n",
        "When to Use: When you see curved patterns in data\n",
        "\n",
        "You have enough data to support higher complexity\n",
        "\n",
        "You're staying within the data range (no extrapolation)\n",
        "\n",
        "Q29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "--->Methods to Evaluate Model Fit When Selecting the Degree of a Polynomial Choosing the right degree of a polynomial is crucial to avoid underfitting (too simple) or overfitting (too complex). Here are the most effective techniques for evaluating model fit and selecting the best polynomial degree:\n",
        "\n",
        "Trainâ€“Test Split Split your data into training and testing sets.\n",
        "Fit models of different degrees on the training set.\n",
        "\n",
        "Evaluate performance on the test set using metrics like:\n",
        "\n",
        "RMSE (Root Mean Squared Error)\n",
        "\n",
        "MAE (Mean Absolute Error)\n",
        "\n",
        "RÂ² score\n",
        "\n",
        "Cross-Validation (K-Fold) Use K-fold cross-validation to evaluate model stability.\n",
        "Average the performance metric (like RMSE or MAE) across all folds for each degree.\n",
        "\n",
        "Plotting Learning Curves Plot:\n",
        "Training error\n",
        "\n",
        "Validation (or test) error\n",
        "\n",
        "For increasing degrees of the polynomial"
      ],
      "metadata": {
        "id": "-AG3W1hi39vz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SvnMDdxN4CTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "for degree in range(1, 6):\n",
        "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "    scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=5)\n",
        "    print(f\"Degree {degree}: CV Error = {-scores.mean():.2f}\")\n"
      ],
      "metadata": {
        "id": "Qfp-kmwT4C1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q30.Why is visualization important in polynomial regression?\n",
        "\n",
        "--->\n",
        "\n",
        "Visualization is essential in polynomial regression to:\n",
        "\n",
        "Understand model behavior\n",
        "\n",
        "Validate degree choice\n",
        "\n",
        "Catch fitting issues\n",
        "\n",
        "Communicate effectively"
      ],
      "metadata": {
        "id": "a6jzq8oS4GEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Suppose X and y are defined\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "X_poly = poly.fit_transform(X)\n",
        "model = LinearRegression().fit(X_poly, y)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, y, color='blue')\n",
        "plt.plot(X, model.predict(X_poly), color='red')\n",
        "plt.title('Polynomial Regression Fit (Degree 3)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KnVNj3BY4aJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q31. How is polynomial regression implemented in Python?\n",
        "\n",
        "--->Polynomial regression is easy to implement in Python using scikit-learn. Here's a step-by-step guide:\n",
        "\n",
        "Import Required Libraries"
      ],
      "metadata": {
        "id": "2yOseCh94cRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n"
      ],
      "metadata": {
        "id": "UwK1Kedm4e7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the Dataset"
      ],
      "metadata": {
        "id": "uHJtj3JI4grx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data (can be replaced with your own)\n",
        "X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)\n",
        "y = np.array([2, 5, 10, 17, 26, 37])\n"
      ],
      "metadata": {
        "id": "l9ZCo3XU4ig9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Polynomial Features and Fit the Model"
      ],
      "metadata": {
        "id": "-2ksxZpR4klB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Degree of the polynomial\n",
        "degree = 2\n",
        "\n",
        "# Create pipeline: Polynomial transformation + Linear Regression\n",
        "model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, y)"
      ],
      "metadata": {
        "id": "x8dQhaWM4mSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make Predictions"
      ],
      "metadata": {
        "id": "EEj_KFl04oO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict values\n",
        "X_pred = np.linspace(1, 6, 100).reshape(-1, 1)\n",
        "y_pred = model.predict(X_pred)"
      ],
      "metadata": {
        "id": "upXYD-nL4pvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the Results"
      ],
      "metadata": {
        "id": "Xr3WOkkO4rbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.scatter(X, y, color='blue', label='Actual data')\n",
        "plt.plot(X_pred, y_pred, color='red', label='Polynomial fit')\n",
        "plt.title('Polynomial Regression (Degree {})'.format(degree))\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HJ4rS82_4tKL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}